{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "grep-euronews-ar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kSm7N4C4cR"
      },
      "source": [
        "# Grep News\n",
        "This code crawle throw [EruoNews](https://euronews.com/) wabsite to extract all the news information and collect them to be saved as **JSON** file.\n",
        "\n",
        "This Code was writtin to collect the data from [Arabic Editoin](https://arabic.euronews.com/) website. \n",
        "\n",
        "tested and validated with Arabic, yet it can work with any other language.\n",
        "To extract the news for the language you want pass the Language paramater\n",
        "\n",
        "the supported Languages:\n",
        "\n",
        "1. [English](https://www.euronews.com) -> www\n",
        "2. [Français](https://fr.euronews.com) -> fr\n",
        "3. [Deutsch](https://de.euronews.com) -> de\n",
        "4. [Italiano](https://it.euronews.com) -> it\n",
        "5. [Español](https://es.euronews.com) -> es\n",
        "6. [Português](https://pt.euronews.com) -> pt\n",
        "7. [Русский](https://ru.euronews.com) -> ru\n",
        "8. [Türkçe](https://tr.euronews.com) -> tr\n",
        "9. [Ελληνικά](https://gr.euronews.com) -> gr\n",
        "10. [Magyar](https://hu.euronews.com) -> hu\n",
        "11. [فارسی](https://per.euronews.com) -> per\n",
        "12. [العربية](https://arabic.euronews.com) -> arabic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e819557e-e2c7-4842-a986-41150c3d959e",
        "_cell_guid": "b18941ed-e9f0-4545-b305-11a72860f9ec",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "JydWK7h7C4cf"
      },
      "source": [
        "#@title importing modules\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import urllib3\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from calendar import monthrange"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BuHtpzbC4cg"
      },
      "source": [
        "**Here** was initialize the http client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "TSj_kgfyC4cg"
      },
      "source": [
        "#@title initial http Client\n",
        "UserAgent = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'}\n",
        "urllib3.disable_warnings()\n",
        "agent = urllib3.PoolManager(headers=UserAgent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "TpEwxq1TC4ch"
      },
      "source": [
        "#@title inital the archive URLs\n",
        "nagency = 'https://{0}.euronews.com'\n",
        "years = [f'{y:04d}' for y in [*range(2001, 2022, 1)]]\n",
        "months = [f'{m:02d}' for m in [*range(1, 13, 1)]]\n",
        "days = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UKQtb0LC4ci"
      },
      "source": [
        "**parserLink** : return the response's page as *BeautifulSoup* object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8lx5dzkRC4ci"
      },
      "source": [
        "#@title ***parserLink*** Function\n",
        "def parserLink(url: str):\n",
        "    page = agent.request('GET', f'{nagency}{url}').data.decode('utf-8')\n",
        "    return BeautifulSoup(page, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_kg_hide-input": true,
        "id": "vnZZF71WC4cj"
      },
      "source": [
        "**getNews** : function validet the page and Extract the news info as a ***JSON*** Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1ca58005-dbea-423c-a524-5a9cb57684a3",
        "_cell_guid": "6054c48f-73b3-4f99-bff4-35b482bbc5e9",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "dlLJMAToC4cj"
      },
      "source": [
        "#@title ***getNews*** Function\n",
        "def getNews(url: str, news=False):\n",
        "    news = parserLink(url).find('main', {'id': 'enw-main-content'})\n",
        "\n",
        "    if news:\n",
        "        if news.find('section', {'class': 'enw-block-error'}):\n",
        "            return None\n",
        "        total_result = int(news.find('section')['data-total-result'])\n",
        "        if not total_result:\n",
        "            return None\n",
        "        jnews = news.find('section', {\n",
        "            'class': 'qa-listingBlock'\n",
        "        }).find('div', {'class': ''})\n",
        "        jnews = json.loads(jnews['data-content'])\n",
        "        if total_result > 30:\n",
        "            paginator = len(news.find('ul', {'class': 'c-paginator'})) - 1\n",
        "            for p in range(2, paginator + 1):\n",
        "                news = parserLink(f'{url}?p={p}').find(\n",
        "                    'main', {'id': 'enw-main-content'})\n",
        "                news = news.find('section', {\n",
        "                    'class': 'qa-listingBlock'\n",
        "                }).find('div', {'class': ''})\n",
        "                jnews.extend(json.loads(news['data-content']))\n",
        "                del news\n",
        "        return jnews\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3E0UtIdC4ck"
      },
      "source": [
        "**eunCrawler** : it gother all the news pages informaton and save it as **JSON** file\n",
        "\n",
        "You can pass one of the following value as string:\n",
        "\n",
        "[www, fr, de, it, es, pt, ru, tr, gr, hu, per, arabic]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IyQ5988MC4cl"
      },
      "source": [
        "#@title ***eunCrawler*** Function \n",
        "## Paramater *language*\n",
        "def eunCrawler(language:str='arabic'):\n",
        "    global nagency\n",
        "    \n",
        "    jNews = []\n",
        "    nagency = nagency.format(language)\n",
        "    yrs = tqdm(years, position=0)\n",
        "    for year in yrs:\n",
        "        yrs.set_description(f'Processing {year} news')\n",
        "        for i, month in enumerate(months):\n",
        "            if i == 1:\n",
        "                dy = monthrange(int(year), int(month))[1]\n",
        "            else:\n",
        "                dy = days[i] + 1\n",
        "            for day in tqdm(range(1, dy),\n",
        "                            desc=f'Processing days {year}/{month}',\n",
        "                            position=1,\n",
        "                            leave=False):\n",
        "                news = getNews(f'/{year}/{month}/{day:02d}/')\n",
        "                if not news:\n",
        "                    continue\n",
        "                jNews.extend(news)\n",
        "                print(end='\\r', flush=True)\n",
        "            print(end='\\r', flush=True)\n",
        "    with open('myfile.json', 'w', encoding='utf8') as jFile:\n",
        "        json.dump(jNews, jFile, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "fAntA6-QC4cm"
      },
      "source": [
        "#@title Run execution\n",
        "eunCrawler()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}